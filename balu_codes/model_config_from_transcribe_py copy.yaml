sample_rate: 16000
log_prediction: true
ctc_reduction: mean_batch
skip_nan_grad: false
a_model_name: #TBD
train_ds:
  manifest_filepath:
  - - /data2/nemo_asr/nemo_asr_set_3.0/bucket1/tarred_audio_manifest.json #TBD
  sample_rate: 16000
  batch_size: 32
  shuffle: true
  num_workers: 4
  pin_memory: true
  use_start_end_token: false
  trim_silence: false
  max_duration: 10.0
  min_duration: 0.1
  is_tarred: false
  tarred_audio_filepaths: null
  shuffle_n: 2048
  bucketing_strategy: synced_randomized
  bucketing_batch_size:
  - 34
  - 30
  - 26
  - 22
  - 18
  - 16
  - 12
  - 8
validation_ds:
  manifest_filepath:
  - /manifests/librispeech/librivox-dev-other.json #TBD
  sample_rate: 16000
  batch_size: 32
  shuffle: false
  num_workers: 8
  pin_memory: true
  use_start_end_token: false
test_ds:
  manifest_filepath:
  - /manifests/librispeech/librivox-dev-other.json #TBD
  sample_rate: 16000
  batch_size: 32
  shuffle: false
  num_workers: 8
  pin_memory: true
  use_start_end_token: false
tokenizer:
  dir: /tokenizers/NeMo_ASR_SET/English/asr_set_3.0/tokenizer_spe_unigram_v128
  type: bpe
  model_path: nemo:e06949b0b85a485e9f280ea6d19e5492_tokenizer.model
  vocab_path: nemo:53bbc634b62446de83525753e95a50ac_vocab.txt
  spe_tokenizer_vocab: nemo:ff63e3c43c5f4b95bff702425366a4a6_tokenizer.vocab
preprocessor:
  _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor
  sample_rate: 16000
  normalize: per_feature
  window_size: 0.025
  window_stride: 0.01
  window: hann
  features: 80
  n_fft: 512
  log: true
  frame_splicing: 1
  dither: 1.0e-05
  pad_to: 0
  pad_value: 0.0
spec_augment:
  _target_: nemo.collections.asr.modules.SpectrogramAugmentation
  freq_masks: 2
  time_masks: 10
  freq_width: 27
  time_width: 0.05
# encoder:
#   _target_: nemo.collections.asr.modules.ConformerEncoder
#   feat_in: 80
#   feat_out: -1
#   n_layers: 18
#   d_model: 512
#   subsampling: striding
#   subsampling_factor: 4
#   subsampling_conv_channels: 512
#   ff_expansion_factor: 4
#   self_attention_model: rel_pos
#   n_heads: 8
#   att_context_size:
#   - -1
#   - -1
#   xscaling: true
#   untie_biases: true
#   pos_emb_max_len: 5000
#   conv_kernel_size: 31
#   conv_norm_type: batch_norm
#   dropout: 0.1
#   dropout_emb: 0.0
#   dropout_att: 0.1

av_enocder:
  d_model: 512
  nhead: 4
  num_layers: 2
  dropout: 0.1

adapters:
  #TBD
  
decoder: # Keep it thse same as you are going by same decoder, note it has dimension.
  _target_: nemo.collections.asr.modules.ConvASRDecoder
  feat_in: 512
  num_classes: 128
  vocabulary:
  - <unk>
  - ▁
  - s
  - t
  - e
  - d
  - o
  - ▁the
  - a
  - i
  - ▁a
  - u
  - 'y'
  - m
  - l
  - 'n'
  - p
  - re
  - c
  - h
  - r
  - ▁s
  - g
  - ▁to
  - er
  - ing
  - f
  - ▁and
  - an
  - ▁i
  - k
  - ▁that
  - ''''
  - ▁of
  - ▁in
  - w
  - ▁p
  - ed
  - or
  - al
  - ar
  - ▁f
  - en
  - in
  - b
  - ▁you
  - ▁w
  - ▁b
  - le
  - ll
  - es
  - ▁it
  - ve
  - ur
  - ▁we
  - ▁re
  - ▁be
  - ly
  - ▁is
  - ▁he
  - ▁o
  - ▁c
  - it
  - ▁n
  - ▁on
  - un
  - ▁t
  - 'on'
  - se
  - th
  - ce
  - ▁do
  - ic
  - ▁for
  - ▁th
  - ion
  - ch
  - ▁was
  - ri
  - ent
  - ▁g
  - ver
  - ▁co
  - li
  - ▁ha
  - ▁ma
  - la
  - ro
  - v
  - us
  - ▁ca
  - ▁di
  - ▁this
  - ra
  - ▁st
  - ▁e
  - ▁not
  - ▁so
  - ▁de
  - ▁have
  - ter
  - ir
  - ▁go
  - ation
  - ▁with
  - ate
  - ▁me
  - ▁mo
  - ment
  - ▁con
  - ▁but
  - vi
  - ▁pro
  - ▁ho
  - j
  - ▁com
  - ight
  - ▁know
  - ▁what
  - ect
  - ▁ex
  - ▁some
  - ▁would
  - ▁like
  - x
  - ▁his
  - q
  - z
optim:
  name: adamw
  lr: 2.0
  betas:
  - 0.9
  - 0.98
  weight_decay: 0.001
  sched:
    name: NoamAnnealing
    d_model: 512
    warmup_steps: 10000
    warmup_ratio: null
    min_lr: 1.0e-06
compute_eval_loss: false
variational_noise:
  start_step: 0
  std: 0.0
target: nemo.collections.asr.models.av_ctc_bpe_models.AV_EncDecCTCModelBPE
nemo_version: 1.9.0rc0
