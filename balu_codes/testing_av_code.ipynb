{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/nemo/NeMo-opensource/nemo/collections/asr/__init__.py\n",
      "/workspace/nemo/NeMo-opensource/nemo/core/__init__.py\n",
      "/workspace/nemo/NeMo-opensource/nemo/__init__.py\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# Insert local paths at the beginning of sys.path\n",
    "sys.path.insert(0, os.path.abspath('/workspace/nemo/NeMo-opensource/'))\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "print(nemo_asr.__file__)\n",
    "import nemo.core as nemo_core\n",
    "print(nemo_core.__file__)\n",
    "from nemo.core import adapter_mixins\n",
    "import nemo\n",
    "print(nemo.__file__)\n",
    "import lightning\n",
    "print(lightning.__file__)\n",
    "# Restore the site-packages paths\n",
    "# sys.path.extend(site_packages_paths)\n",
    "\n",
    "import torch\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from pytorch_lightning import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(project=\"NEMO_TEST\")\n",
    "# import nemo.collections.asr as nemo_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-08 23:55:14 collections:317] Dataset loaded with 18351 files totalling 50.98 hours\n",
      "[NeMo I 2024-07-08 23:55:14 collections:319] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2024-07-08 23:55:14 collections:317] Dataset loaded with 18351 files totalling 50.98 hours\n",
      "[NeMo I 2024-07-08 23:55:14 collections:319] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2024-07-08 23:55:15 collections:317] Dataset loaded with 18351 files totalling 50.98 hours\n",
      "[NeMo I 2024-07-08 23:55:15 collections:319] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2024-07-08 23:55:15 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_2.0.0rc0/QuartzNet15x5Base-En/2b066be39e9294d7100fb176ec817722/QuartzNet15x5Base-En.nemo.\n",
      "[NeMo I 2024-07-08 23:55:15 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_2.0.0rc0/QuartzNet15x5Base-En/2b066be39e9294d7100fb176ec817722/QuartzNet15x5Base-En.nemo\n",
      "[NeMo I 2024-07-08 23:55:15 common:815] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2024-07-08 23:55:16 features:305] PADDING: 16\n",
      "[NeMo I 2024-07-08 23:55:17 save_restore_connector:263] Model EncDecCTCModel was successfully restored from /root/.cache/torch/NeMo/NeMo_2.0.0rc0/QuartzNet15x5Base-En/2b066be39e9294d7100fb176ec817722/QuartzNet15x5Base-En.nemo.\n"
     ]
    }
   ],
   "source": [
    "TRAIN_MANIFEST = \"/disk1/it1/annotations/manifest_train.json\"\n",
    "TEST_MANIFEST = \"/disk1/it1/annotations/manifest_train.json\"\n",
    "override_config_file_path = \"/workspace/nemo/NeMo-opensource/balu_codes/ctc_model_QuartzNet15x5Base copy.yaml\"\n",
    "conf = OmegaConf.load(override_config_file_path)\n",
    "OmegaConf.set_struct(conf, True)\n",
    "model = nemo_asr.models.AV_EncDecCTCModel(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-08 23:55:18 collections:317] Dataset loaded with 18351 files totalling 50.98 hours\n",
      "[NeMo I 2024-07-08 23:55:18 collections:319] 0 files were filtered totalling 0.00 hours\n"
     ]
    }
   ],
   "source": [
    "with open_dict(model.cfg):\n",
    "  # Train Dataloader\n",
    "  model.cfg.train_ds.manifest_filepath = TRAIN_MANIFEST\n",
    "  model.cfg.train_ds.batch_size = 32\n",
    "  model.cfg.train_ds.is_tarred = False\n",
    "  model.cfg.train_ds.tarred_audio_filepaths = None\n",
    "\n",
    "  model.cfg.validation_ds.manifest_filepath = TEST_MANIFEST\n",
    "  model.cfg.validation_ds.batch_size = 32\n",
    "\n",
    "model.setup_training_data(model.cfg.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summarize()\n",
    "model.freeze()\n",
    "# model.summarize()\n",
    "modules_to_train = [model.a_linear, model.v_linear, model.av_encoder, model.av_enocder_layer, model.a_modal_embs\n",
    "                    , model.v_modal_embs, model.decoder]\n",
    "for module in modules_to_train:\n",
    "    module.train()\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "\n",
    "# model.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: GPU available: True (cuda), used: True\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0708 23:55:19.239430 134716664817472 rank_zero.py:64] GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "I0708 23:55:19.266383 134716664817472 rank_zero.py:64] TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "I0708 23:55:19.267290 134716664817472 rank_zero.py:64] HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-08 23:55:19 exp_manager:396] Experiments will be logged at test_experiments/test_wpe_quartz/2024-07-08_23-55-19\n",
      "[NeMo I 2024-07-08 23:55:19 exp_manager:856] TensorboardLogger has been set up\n",
      "[NeMo I 2024-07-08 23:55:19 exp_manager:871] WandBLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-07-08 23:55:19 exp_manager:966] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 3000. Please ensure that max_steps will run for at least 3 epochs to ensure that checkpointing will not error out.\n"
     ]
    }
   ],
   "source": [
    "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "max_steps = 3000\n",
    "\n",
    "trainer = Trainer(devices=1, accelerator=accelerator, max_steps=max_steps,\n",
    "                  enable_checkpointing=False, logger=False,\n",
    "                  log_every_n_steps=5, check_val_every_n_epoch=3)\n",
    "\n",
    "model.set_trainer(trainer)\n",
    "\n",
    "# Prepare NeMo's Experiment manager to handle checkpoint saving and logging for us\n",
    "from nemo.utils import exp_manager\n",
    "\n",
    "\n",
    "# Environment variable generally used for multi-node multi-gpu training.\n",
    "# In notebook environments, this flag is unnecessary and can cause logs of multiple training runs to overwrite each other.\n",
    "os.environ.pop('NEMO_EXPM_VERSION', None)\n",
    "\n",
    "exp_config = exp_manager.ExpManagerConfig(\n",
    "    exp_dir=f'test_experiments/',\n",
    "    name=f\"test_wpe_quartz\",\n",
    "    checkpoint_callback_params=exp_manager.CallbackParams(\n",
    "        monitor=\"val_wer\",\n",
    "        mode=\"min\",\n",
    "        always_save_nemo=True,\n",
    "        save_best_model=True,\n",
    "    ),\n",
    "    create_wandb_logger=True,\n",
    "    wandb_logger_kwargs=OmegaConf.create({\"project\": \"NEMO_TEST\", \"name\": \"test_wpe_quartz\", \"log_model\":\"all\"}),\n",
    ")\n",
    "\n",
    "exp_config = OmegaConf.structured(exp_config)\n",
    "\n",
    "logdir = exp_manager.exp_manager(trainer, exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0708 23:55:20.059351 134716664817472 jupyter.py:224] Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlakshmipathi-balaji\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>test_experiments/wandb/run-20240708_235520-2024-07-08_23-55-19</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lakshmipathi-balaji/NEMO_TEST/runs/2024-07-08_23-55-19' target=\"_blank\">test_wpe_quartz</a></strong> to <a href='https://wandb.ai/lakshmipathi-balaji/NEMO_TEST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lakshmipathi-balaji/NEMO_TEST' target=\"_blank\">https://wandb.ai/lakshmipathi-balaji/NEMO_TEST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lakshmipathi-balaji/NEMO_TEST/runs/2024-07-08_23-55-19' target=\"_blank\">https://wandb.ai/lakshmipathi-balaji/NEMO_TEST/runs/2024-07-08_23-55-19</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-08 23:55:21 modelPT:767] Optimizer config = Novograd (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.8, 0.5]\n",
      "        eps: 1e-08\n",
      "        grad_averaging: False\n",
      "        lr: 0.01\n",
      "        weight_decay: 0.001\n",
      "    )\n",
      "[NeMo I 2024-07-08 23:55:21 lr_scheduler:772] Scheduler not initialized as no `sched` config supplied to setup_optimizer()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name             | Type                    | Params | Mode \n",
      "---------------------------------------------------------------------\n",
      "0 | a_model          | EncDecCTCModel          | 18.9 M | eval \n",
      "1 | a_linear         | Linear                  | 524 K  | train\n",
      "2 | v_linear         | Linear                  | 262 K  | train\n",
      "3 | av_enocder_layer | TransformerEncoderLayer | 3.2 M  | train\n",
      "4 | av_encoder       | TransformerEncoder      | 6.3 M  | train\n",
      "5 | a_modal_embs     | Embedding               | 512    | train\n",
      "6 | v_modal_embs     | Embedding               | 512    | train\n",
      "7 | decoder          | ConvASRDecoder          | 14.9 K | train\n",
      "8 | loss             | CTCLoss                 | 0      | eval \n",
      "9 | wer              | WER                     | 0      | eval \n",
      "---------------------------------------------------------------------\n",
      "10.3 M    Trainable params\n",
      "18.9 M    Non-trainable params\n",
      "29.2 M    Total params\n",
      "116.740   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea39ad975454155a69aa466a018c9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-07-08 23:55:21 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-07-08 23:55:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a81cca20ec4a6a8abc39f8a4b882dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-08 23:55:22 preemption:56] Preemption requires torch distributed to be initialized, disabling preemption\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ec198837cb47a5a0d217f8bba173c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 2, global step 1722: 'val_wer' reached 0.26906 (best 0.26906), saving model to '/workspace/nemo/NeMo-opensource/balu_codes/test_experiments/test_wpe_quartz/2024-07-08_23-55-19/checkpoints/test_wpe_quartz--val_wer=0.2691-epoch=2.ckpt' as top 3\n",
      "I0709 00:11:43.277340 134716664817472 rank_zero.py:64] Epoch 2, global step 1722: 'val_wer' reached 0.26906 (best 0.26906), saving model to '/workspace/nemo/NeMo-opensource/balu_codes/test_experiments/test_wpe_quartz/2024-07-08_23-55-19/checkpoints/test_wpe_quartz--val_wer=0.2691-epoch=2.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-07-09 00:11:50 nemo_model_checkpoint:217] New best .nemo model saved to: /workspace/nemo/NeMo-opensource/balu_codes/test_experiments/test_wpe_quartz/2024-07-08_23-55-19/checkpoints/test_wpe_quartz.nemo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_steps=3000` reached.\n",
      "I0709 00:18:02.561257 134716664817472 rank_zero.py:64] `Trainer.fit` stopped: `max_steps=3000` reached.\n",
      "INFO: Restoring states from the checkpoint path at /workspace/nemo/NeMo-opensource/balu_codes/test_experiments/test_wpe_quartz/2024-07-08_23-55-19/checkpoints/test_wpe_quartz--val_wer=0.2691-epoch=2.ckpt\n",
      "I0709 00:18:04.404496 134716664817472 rank_zero.py:64] Restoring states from the checkpoint path at /workspace/nemo/NeMo-opensource/balu_codes/test_experiments/test_wpe_quartz/2024-07-08_23-55-19/checkpoints/test_wpe_quartz--val_wer=0.2691-epoch=2.ckpt\n",
      "INFO: Restored all states from the checkpoint at /workspace/nemo/NeMo-opensource/balu_codes/test_experiments/test_wpe_quartz/2024-07-08_23-55-19/checkpoints/test_wpe_quartz--val_wer=0.2691-epoch=2.ckpt\n",
      "I0709 00:18:04.564626 134716664817472 rank_zero.py:64] Restored all states from the checkpoint at /workspace/nemo/NeMo-opensource/balu_codes/test_experiments/test_wpe_quartz/2024-07-08_23-55-19/checkpoints/test_wpe_quartz--val_wer=0.2691-epoch=2.ckpt\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
