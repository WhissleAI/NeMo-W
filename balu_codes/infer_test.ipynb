{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For valdation with given model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/home/bld56/gsoc/nemo/NeMo-opensource/'))\n",
    "import nemo.core as nemo_core\n",
    "from nemo.core import adapter_mixins\n",
    "from nemo.utils import exp_manager\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import nemo\n",
    "import json\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torchmetrics.text import WordErrorRate\n",
    "import warnings\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_configure_model(config_file_path):\n",
    "    conf = OmegaConf.load(config_file_path)\n",
    "    overrides = OmegaConf.from_cli()\n",
    "    updated_conf = OmegaConf.merge(conf, overrides)\n",
    "    OmegaConf.set_struct(updated_conf, True)\n",
    "    model = nemo_asr.models.AV_EncDecCTCModelBPE(updated_conf)\n",
    "\n",
    "    model.setup_training_data(model.cfg.train_ds)\n",
    "    return model, conf\n",
    "\n",
    "# Function to freeze and unfreeze model parameters based on adapters\n",
    "def manage_model_adapters(model, conf):\n",
    "    # Freeze the entire model\n",
    "    model.freeze()\n",
    "    \n",
    "    # Determine which modules to train based on configuration\n",
    "    if model.cfg.use_video_modality:\n",
    "        modules_to_train = [\n",
    "            model.a_linear, model.v_linear, model.av_encoder, model.av_enocder_layer, \n",
    "            model.a_modal_embs, model.v_modal_embs, model.decoder, model.a_pos_enc, model.v_pos_enc\n",
    "        ]\n",
    "    elif not model.cfg.use_video_modality and model.cfg.use_pretrained_dec:\n",
    "        modules_to_train = [model.a_model.decoder]\n",
    "    else:  # not model.cfg.use_video_modality and not model.cfg.use_pretrained_dec\n",
    "        modules_to_train = [model.decoder]\n",
    "    \n",
    "    # Set the selected modules to training mode and enable gradients\n",
    "    for module in modules_to_train:\n",
    "        module.train()\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Handle adapter configurations if needed\n",
    "    if conf.adapters.linear_adapter.keep:\n",
    "        model.a_model.freeze()\n",
    "        model.a_model.set_enabled_adapters(enabled=False)\n",
    "        model.a_model.set_enabled_adapters(name=conf.adapters.linear_adapter.name, enabled=True)\n",
    "        model.a_model.unfreeze_enabled_adapters()\n",
    "    else:\n",
    "        model.a_model.unfreeze()\n",
    "\n",
    "# Function to set up the trainer\n",
    "def setup_trainer():\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "    trainer = Trainer(\n",
    "        devices=1, accelerator=accelerator, \n",
    "        # strategy=\"ddp_find_unused_parameters_true\",\n",
    "        # strategy=\"ddp_notebook\",\n",
    "        max_epochs=100,\n",
    "        enable_checkpointing=False, logger=False,\n",
    "        log_every_n_steps=5, check_val_every_n_epoch=1,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "# Function to set up experiment manager\n",
    "def setup_exp_manager(trainer, model):\n",
    "    os.environ.pop('NEMO_EXPM_VERSION', None)\n",
    "\n",
    "    exp_config = exp_manager.ExpManagerConfig(\n",
    "        exp_dir=model.cfg.exp_dir,\n",
    "        name=f'{model.cfg.wandb.run_name}',\n",
    "        checkpoint_callback_params=exp_manager.CallbackParams(\n",
    "            monitor=\"val_u_wer\",\n",
    "            mode=\"min\",\n",
    "            always_save_nemo=True,\n",
    "            save_best_model=True,\n",
    "        ),\n",
    "        create_wandb_logger=model.cfg.wandb.create_wandb_logger,\n",
    "        wandb_logger_kwargs=OmegaConf.create({\"project\": f\"{model.cfg.wandb.project}\", \"name\": f\"{model.cfg.wandb.run_name}_{model.cfg.train_ds.override_snr_ratio}\", \"log_model\": model.cfg.wandb.log_model}),\n",
    "    )\n",
    "\n",
    "    exp_config = OmegaConf.structured(exp_config)\n",
    "    logdir = exp_manager.exp_manager(trainer, exp_config)\n",
    "    if model.cfg.wandb.create_wandb_logger:\n",
    "        trainer.loggers[1].log_hyperparams(OmegaConf.to_container(model.cfg)) # wandb logger\n",
    "        # log the manifest file to wandb server\n",
    "        trainer.loggers[1].experiment.log_artifact(f\"{model.cfg.train_ds.manifest_filepath}\")\n",
    "        trainer.loggers[1].experiment.log_artifact(f\"{model.cfg.validation_ds.manifest_filepath}\")\n",
    "        \n",
    "    return logdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-08-25 15:44:38 mixins:172] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-08-25 15:44:38 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath:\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket1/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket2/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket3/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket4/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket5/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket6/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket7/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket8/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 1\n",
      "    shuffle: true\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket1/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket2/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket3/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket4/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket5/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket6/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket7/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket8/audio__OP_0..8191_CL_.tar\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size:\n",
      "    - 34\n",
      "    - 30\n",
      "    - 26\n",
      "    - 22\n",
      "    - 18\n",
      "    - 16\n",
      "    - 12\n",
      "    - 8\n",
      "    \n",
      "[NeMo W 2024-08-25 15:44:38 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /manifests/librispeech/librivox-dev-other.json\n",
      "    - /manifests/librispeech/librivox-dev-clean.json\n",
      "    - /manifests/librispeech/librivox-test-other.json\n",
      "    - /manifests/librispeech/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2024-08-25 15:44:38 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /manifests/librispeech/librivox-dev-other.json\n",
      "    - /manifests/librispeech/librivox-dev-clean.json\n",
      "    - /manifests/librispeech/librivox-test-other.json\n",
      "    - /manifests/librispeech/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-08-25 15:44:38 features:305] PADDING: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-08-25 15:44:39 save_restore_connector:263] Model EncDecCTCModelBPE was successfully restored from /home/bld56/.cache/torch/NeMo/NeMo_2.0.0rc0/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo.\n",
      "EncDecCTCModelBPE(\n",
      "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
      "    (featurizer): FilterbankFeatures()\n",
      "  )\n",
      "  (encoder): ConformerEncoder(\n",
      "    (pre_encode): ConvSubsampling(\n",
      "      (out): Linear(in_features=10240, out_features=512, bias=True)\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (pos_enc): RelPositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x ConformerLayer(\n",
      "        (norm_feed_forward1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward1): ConformerFeedForward(\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (activation): Swish()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): ConformerConvolution(\n",
      "          (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): CausalConv1D(512, 512, kernel_size=(31,), stride=(1,), groups=512)\n",
      "          (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): Swish()\n",
      "          (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (norm_self_att): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attn): RelPositionMultiHeadAttention(\n",
      "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (norm_feed_forward2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward2): ConformerFeedForward(\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (activation): Swish()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ConvASRDecoder(\n",
      "    (decoder_layers): Sequential(\n",
      "      (0): Conv1d(512, 129, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (loss): CTCLoss()\n",
      "  (spec_augmentation): SpectrogramAugmentation(\n",
      "    (spec_augment): SpecAugment()\n",
      "  )\n",
      "  (wer): WER()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Main function to execute the workflow\n",
    "# def main(config_file_path, args):\n",
    "# config_file_path = '/home/bld56/gsoc/nemo/NeMo-opensource/balu_codes/configs/c1.yaml'\n",
    "# model, conf = load_and_configure_model(config_file_path)\n",
    "# ckpt_path = f\"/tmp/bld56_dataset_v1/saved_models/pre_av_ndec_uman_ntok--val_u_wer=0.0809-epoch=11.ckpt\"\n",
    "ckpt_path = f\"/home/bld56/.cache/torch/NeMo/NeMo_2.0.0rc0/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo\"\n",
    "model = nemo_asr.models.AV_EncDecCTCModelBPE.restore_from(ckpt_path, override_config_path=None) \n",
    "model.cfg.train_ds.manifest_filepath = '/tmp/bld56_dataset_v1/it2/annotations/manifest_train_no_label.json'\n",
    "model.cfg.validation_ds.manifest_filepath = '/tmp/bld56_dataset_v1/it2/annotations/manifest_eval_no_label.json'\n",
    "model.cfg.test_ds.manifest_filepath = '/tmp/bld56_dataset_v1/it2/annotations/manifest_test_no_label.json'\n",
    "print(model)\n",
    "# model.cfg.wandb.run_name += 'pre+'\n",
    "# manage_model_adapters(model, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-08-25 15:44:39 nemo_logging:349] /home/bld56/.miniconda3/envs/nemo/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/bld56/.miniconda3/envs/nemo/lib/python3.10/sit ...\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = setup_trainer()\n",
    "model.set_trainer(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-08-25 15:46:29 nemo_logging:349] /home/bld56/.miniconda3/envs/nemo/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/bld56/.miniconda3/envs/nemo/lib/python3.10/sit ...\n",
      "    \n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "[NeMo W 2024-08-25 15:46:29 nemo_logging:349] /home/bld56/.miniconda3/envs/nemo/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:105: Total length of `list` across ranks is zero. Please make sure this was your intention.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Aug 16 Weekly meet to develop to transcribe fucniton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/home/bld56/gsoc/nemo/NeMo-opensource/'))\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import json\n",
    "import nemo.collections.asr.data.av_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load the model from a .nemo file\n",
    "def load_model(nemo_file_path):\n",
    "    model = nemo_asr.models.AV_EncDecCTCModelBPE.restore_from(nemo_file_path)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Function to perform inference on a single sample\n",
    "def infer_single_sample(model, sample):\n",
    "    # Prepare input data\n",
    "    audio_file = sample['audio_filepath']\n",
    "    video_file = sample['video_filepath']\n",
    "    feature_file = sample['feature_file']\n",
    "    duration = sample['duration']\n",
    "    \n",
    "    # Perform inference\n",
    "    transcription = model.transcribe(\n",
    "        audio=[audio_file],\n",
    "        return_hypotheses = True,\n",
    "        override_duration = duration,\n",
    "    )\n",
    "    \n",
    "    return transcription[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Load the tokenizer model from the specified path\n",
    "def load_tokenizer(tokenizer_model_path):\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.load(tokenizer_model_path)\n",
    "    return tokenizer\n",
    "\n",
    "# tokenizer_path = \"/home/bld56/gsoc/nemo/NeMo-opensource/tutorials/asr/tokenizers/av_tokenizer/final_tokenizer/tokenizer.model\"\n",
    "# for i in range(self.tokenizer.vocab_size):\n",
    "#         piece = self.tokenizer.ids_to_tokens([i])\n",
    "#         piece = piece[0]\n",
    "#         vocabulary[piece] = i + 1\n",
    "# tokenizer = load_tokenizer(tokenizer_path)\n",
    "config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-08-17 12:36:32 mixins:172] Tokenizer SentencePieceTokenizer initialized with 356 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-08-17 12:36:32 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /tmp/bld56_dataset_v1/it2/annotations/manifest_train.json\n",
      "    video_frame_rate: 5\n",
      "    get_vid_feats: true\n",
      "    get_zero_vid_feats: false\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: true\n",
      "    num_workers: 11\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: synced_randomized\n",
      "    override_snr_ratio: 0.7\n",
      "    bucketing_batch_size:\n",
      "    - 34\n",
      "    - 30\n",
      "    - 26\n",
      "    - 22\n",
      "    - 18\n",
      "    - 16\n",
      "    - 12\n",
      "    - 8\n",
      "    \n",
      "[NeMo W 2024-08-17 12:36:32 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /tmp/bld56_dataset_v1/it2/annotations/manifest_eval.json\n",
      "    video_frame_rate: 5\n",
      "    get_vid_feats: true\n",
      "    get_zero_vid_feats: false\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 11\n",
      "    pin_memory: true\n",
      "    override_snr_ratio: 0.7\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2024-08-17 12:36:32 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: /tmp/bld56_dataset_v1/it2/annotations/manifest_eval.json\n",
      "    video_frame_rate: 5\n",
      "    get_vid_feats: true\n",
      "    get_zero_vid_feats: false\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 11\n",
      "    pin_memory: true\n",
      "    override_snr_ratio: 0.7\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-08-17 12:36:32 cloud:58] Found existing object /home/bld56/.cache/torch/NeMo/NeMo_2.0.0rc0/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo.\n",
      "[NeMo I 2024-08-17 12:36:32 cloud:64] Re-using file from: /home/bld56/.cache/torch/NeMo/NeMo_2.0.0rc0/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo\n",
      "[NeMo I 2024-08-17 12:36:32 common:815] Instantiating model from pre-trained checkpoint\n",
      "Updated encoder _target_ model : nemo.collections.asr.modules.conformer_encoder.ConformerEncoderAdapter\n",
      "[NeMo I 2024-08-17 12:36:32 cloud:58] Found existing object /home/bld56/.cache/torch/NeMo/NeMo_2.0.0rc0/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo.\n",
      "[NeMo I 2024-08-17 12:36:32 cloud:64] Re-using file from: /home/bld56/.cache/torch/NeMo/NeMo_2.0.0rc0/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo\n",
      "[NeMo I 2024-08-17 12:36:32 common:815] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2024-08-17 12:36:33 mixins:172] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-08-17 12:36:33 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath:\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket1/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket2/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket3/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket4/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket5/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket6/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket7/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket8/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 1\n",
      "    shuffle: true\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket1/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket2/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket3/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket4/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket5/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket6/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket7/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket8/audio__OP_0..8191_CL_.tar\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size:\n",
      "    - 34\n",
      "    - 30\n",
      "    - 26\n",
      "    - 22\n",
      "    - 18\n",
      "    - 16\n",
      "    - 12\n",
      "    - 8\n",
      "    \n",
      "[NeMo W 2024-08-17 12:36:33 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /manifests/librispeech/librivox-dev-other.json\n",
      "    - /manifests/librispeech/librivox-dev-clean.json\n",
      "    - /manifests/librispeech/librivox-test-other.json\n",
      "    - /manifests/librispeech/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2024-08-17 12:36:33 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /manifests/librispeech/librivox-dev-other.json\n",
      "    - /manifests/librispeech/librivox-dev-clean.json\n",
      "    - /manifests/librispeech/librivox-test-other.json\n",
      "    - /manifests/librispeech/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-08-17 12:36:33 features:305] PADDING: 0\n",
      "[NeMo I 2024-08-17 12:36:34 save_restore_connector:263] Model EncDecCTCModelBPE was successfully restored from /home/bld56/.cache/torch/NeMo/NeMo_2.0.0rc0/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo.\n",
      "[NeMo I 2024-08-17 12:36:34 save_restore_connector:263] Model AV_EncDecCTCModelBPE was successfully restored from /tmp/bld56_dataset_v1/tmp/av_ndec_lman_ntok_0.5/2024-08-16_11-16-34/checkpoints/av_ndec_lman_ntok_0.5.nemo.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AV_EncDecCTCModelBPE(\n",
       "  (a_model): EncDecCTCModelBPE(\n",
       "    (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "      (featurizer): FilterbankFeatures()\n",
       "    )\n",
       "    (encoder): ConformerEncoderAdapter(\n",
       "      (pre_encode): ConvSubsampling(\n",
       "        (out): Linear(in_features=10240, out_features=512, bias=True)\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (pos_enc): RelPositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-17): 18 x ConformerLayer(\n",
       "          (norm_feed_forward1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward1): ConformerFeedForward(\n",
       "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (activation): Swish()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (conv): ConformerConvolution(\n",
       "            (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (depthwise_conv): CausalConv1D(512, 512, kernel_size=(31,), stride=(1,), groups=512)\n",
       "            (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): Swish()\n",
       "            (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norm_self_att): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): RelPositionMultiHeadAttention(\n",
       "            (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (norm_feed_forward2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward2): ConformerFeedForward(\n",
       "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (activation): Swish()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (adapter_layer): ModuleDict(\n",
       "            (AV_v1): LinearAdapter(\n",
       "              (module): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=512, out_features=64, bias=False)\n",
       "                (2): SiLU(inplace=True)\n",
       "                (3): Linear(in_features=64, out_features=512, bias=False)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): ConvASRDecoder(\n",
       "      (decoder_layers): Sequential(\n",
       "        (0): Conv1d(512, 129, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (loss): CTCLoss()\n",
       "    (spec_augmentation): SpectrogramAugmentation(\n",
       "      (spec_augment): SpecAugment()\n",
       "    )\n",
       "    (wer): WER()\n",
       "  )\n",
       "  (a_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (v_linear): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (av_enocder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (av_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (a_modal_embs): Embedding(1, 512)\n",
       "  (v_modal_embs): Embedding(1, 512)\n",
       "  (a_pos_enc): Embedding(10000, 512)\n",
       "  (v_pos_enc): Embedding(10000, 512)\n",
       "  (decoder): ConvASRDecoder(\n",
       "    (decoder_layers): Sequential(\n",
       "      (0): Conv1d(512, 357, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (loss): CTCLoss()\n",
       "  (wer): AV_WER()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifest_file_path = '/tmp/bld56_dataset_v1/it2/annotations/manifest_eval.json'  # Path to your input manifest file\n",
    "nemo_file_path = '/tmp/bld56_dataset_v1/tmp/av_ndec_lman_ntok_0.5/2024-08-16_11-16-34/checkpoints/av_ndec_lman_ntok_0.5.nemo'  # Path to your trained .nemo file\n",
    "output_file_path = 'temp.json'  # Path to save the inference results\n",
    "model = load_model(nemo_file_path)\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-08-17 12:36:34 collections:321] Dataset loaded with 2200 files totalling 6.11 hours\n",
      "[NeMo I 2024-08-17 12:36:34 collections:323] 0 files were filtered totalling 0.00 hours\n"
     ]
    }
   ],
   "source": [
    "dataset = nemo.collections.asr.data.av_to_text.AVToBPEDataset(\n",
    "        manifest_filepath='/tmp/bld56_dataset_v1/it2/annotations/manifest_eval.json',\n",
    "        tokenizer= model.tokenizer,\n",
    "        sample_rate= 16000,\n",
    "        int_values=config.get('int_values', False),\n",
    "        max_duration=config.get('max_duration', None),\n",
    "        min_duration=config.get('min_duration', None),\n",
    "        max_utts=config.get('max_utts', 0),\n",
    "        trim=config.get('trim_silence', False),\n",
    "        use_start_end_token=config.get('use_start_end_token', True),\n",
    "        return_sample_id=config.get('return_sample_id', False),\n",
    "        channel_selector=config.get('channel_selector', None),\n",
    "        video_frame_rate=config.get('video_frame_rate', 5),\n",
    "        get_vid_feats=config.get('get_vid_feats', True),\n",
    "        get_zero_vid_feats = config.get('get_zero_vid_feats', False),\n",
    "        override_snr_ratio = config.get('override_snr_ratio', None),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch_size = 1\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so regular and complete a part of normal everyday living that finding newspapers on the news then buying them morning and night was takenso regular and complete a part of normal everyday living that finding newspapers on the news then buying them morning and night was taken <N11>\n",
      "\n",
      "\n",
      "<N228>+<N228>+<N228>+<N228>+<N228>+<N228>+ re reguular and and com compleletee a p parart of of n normmalal eververyy dayay l liivving that that f findding n nwssppaapperss on on the neewssstandnds b buyying the them m mororninging and and n niightt<N228>+ wasas t taakinging<N228>+<N228>+<N228>+rereggullar and and compleletee a a parart of of norormmal e eververy d dayay l liivving that that f findinging n nwssppaapperss on the the neewssstandnds b buyyinging themm m mornning and and<N228>+ n nighght<N228>+ wasas t t takk<N228>+ing<N228>+ <N36>ararararllararararararararlararararararararararararararararararararararlararararararararlararar\n"
     ]
    }
   ],
   "source": [
    "signal, signal_len, video_input_signal, transcript, transcript_len = dataloader.__iter__().__next__()\n",
    "log_probs, encoded_len, predictions = model.forward(audio_input_signal=signal, audio_input_signal_length=signal_len, video_input_signal=video_input_signal)\n",
    "loss_value = model.loss(\n",
    "            log_probs=log_probs, targets=transcript, input_lengths=encoded_len, target_lengths=transcript_len\n",
    "        )\n",
    "# print(transcript, predictions)\n",
    "# tokenizer = load_tokenizer('/home/bld56/gsoc/nemo/NeMo-opensource/tutorials/asr/tokenizers/av_tokenizer/final_toknizer/tokenizer.model')\n",
    "# model.wer.decoding.decode_tokens_to_str(predictions[0].cpu().numpy().tolist())\n",
    "# replace predictions[0] where 356 to 355\n",
    "predictions[0][predictions[0] == 356] = 355\n",
    "print(model.wer.decoding.decode_tokens_to_str(transcript[0].cpu().numpy().tolist()))\n",
    "print('\\n')\n",
    "print(model.wer.decoding.decode_tokens_to_str(predictions[0].cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<N228>+<N228>+<N228>+<N228>+<N228>+<N228>+ re reguular and and com compleletee a p parart of of n normmalal eververyy dayay l liivving that that f findding n nwssppaapperss on on the neewssstandnds b buyying the them m mororninging and and n niightt<N228>+ wasas t taakinging<N228>+<N228>+<N228>+rereggullar and and compleletee a a parart of of norormmal e eververy d dayay l liivving that that f findinging n nwssppaapperss on the the neewssstandnds b buyyinging themm m mornning and and<N228>+ n nighght<N228>+ wasas t t takk<N228>+ing<N228>+ ararararllararararararararlararararararararararararararararararararararlararararararararlararar\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "temp_str = model.wer.decoding.decode_tokens_to_str(predictions[0].cpu().numpy().tolist())\n",
    "r_tags = re.findall(r'<N\\d+>', temp_str)\n",
    "for tag in r_tags:\n",
    "    unlabelled_h = temp_str.replace(tag, '')\n",
    "print(unlabelled_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to run inference on a manifest file\n",
    "def run_inference(manifest_file_path, nemo_file_path, output_file_path):\n",
    "    # Load the model\n",
    "    model = load_model(nemo_file_path)\n",
    "    \n",
    "    # Read the manifest file\n",
    "    with open(manifest_file_path, 'r') as f:\n",
    "        manifest_data = [json.loads(line.strip()) for line in f]\n",
    "    \n",
    "    # Run inference on each sample in the manifest\n",
    "    results = []\n",
    "    for sample in manifest_data:\n",
    "        transcription = infer_single_sample(model, sample)\n",
    "        result = {\n",
    "            'audio_filepath': sample['audio_filepath'],\n",
    "            'video_filepath': sample['video_filepath'],\n",
    "            'feature_file': sample['feature_file'],\n",
    "            'duration': sample['duration'],\n",
    "            'transcription': transcription\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    # Save the results to the output file\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for result in results:\n",
    "            f.write(json.dumps(result) + '\\n')\n",
    "\n",
    "    print(f\"Inference completed. Results saved to {output_file_path}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    manifest_file_path = '/tmp/bld56_dataset_v1/it2/annotations/manifest_eval.json'  # Path to your input manifest file\n",
    "    nemo_file_path = '/tmp/bld56_dataset_v1/tmp/av_ndec_lman_ntok_0.5/2024-08-16_11-16-34/checkpoints/av_ndec_lman_ntok_0.5.nemo'  # Path to your trained .nemo file\n",
    "    output_file_path = 'temp.json'  # Path to save the inference results\n",
    "    \n",
    "    run_inference(manifest_file_path, nemo_file_path, output_file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
